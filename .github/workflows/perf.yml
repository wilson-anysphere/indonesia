name: perf

on:
  pull_request:
  push:
    branches:
      - main

permissions:
  contents: read
  actions: read

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  perf:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    env:
      # Use a shared target dir so the baseline + current worktrees can reuse build artifacts.
      CARGO_TARGET_DIR: ${{ github.workspace }}/target
      # Improves CI triage when `nova` panics or `anyhow` captures a backtrace.
      RUST_BACKTRACE: "1"

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          # Pin the toolchain so cached `main` baselines remain comparable over time.
          toolchain: 1.92.0

      - name: Rust cache
        uses: Swatinem/rust-cache@v2

      # Optional optimization: reuse a cached baseline run from the `perf` workflow on `main`
      # instead of benching the base commit via a worktree.
      - name: Find cached baseline run (main)
        if: github.event_name == 'pull_request' && github.event.pull_request.base.ref == 'main' && github.event.pull_request.head.repo.full_name == github.repository
        id: baseline-run
        env:
          GH_TOKEN: ${{ github.token }}
          BASE_SHA: ${{ github.event.pull_request.base.sha }}
        run: |
          set -euo pipefail
          api_url="${GITHUB_API_URL}/repos/${GITHUB_REPOSITORY}/actions/workflows/perf.yml/runs?branch=main&event=push&status=completed&per_page=100"
          run_id="$(
            curl -fsSL \
              -H "Authorization: Bearer ${GH_TOKEN}" \
              -H "Accept: application/vnd.github+json" \
              "${api_url}" | \
              python3 -c 'import json,os,sys; base=os.environ["BASE_SHA"]; data=json.load(sys.stdin); print(next((str(run["id"]) for run in data.get("workflow_runs", []) if run.get("head_sha")==base and run.get("conclusion")=="success"), ""), end="")' \
              || true
          )"

          if [ -n "${run_id}" ]; then
            echo "run_id=${run_id}" >> "${GITHUB_OUTPUT}"
          else
            echo "No cached baseline artifact found for ${BASE_SHA}; benching base commit instead."
          fi

      - name: Download cached baseline artifact (main)
        if: github.event_name == 'pull_request' && steps.baseline-run.outputs.run_id != ''
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: perf-baseline-main
          path: baseline
          repository: ${{ github.repository }}
          run-id: ${{ steps.baseline-run.outputs.run_id }}

      - name: Prepare baseline from artifact
        if: github.event_name == 'pull_request'
        env:
          BASELINE_RUN_ID: ${{ steps.baseline-run.outputs.run_id }}
        run: |
          set -euxo pipefail
          if [ -s baseline/perf-current.json ]; then
            cp baseline/perf-current.json perf-base.json
            if python3 -c 'import json; json.load(open("perf-base.json"))' >/dev/null 2>&1; then
              echo "BASELINE_SOURCE=perf-baseline-main (run-id ${BASELINE_RUN_ID})" >> "${GITHUB_ENV}"
            else
              echo "Downloaded baseline perf-current.json is not valid JSON; falling back to worktree bench." >&2
              rm -f perf-base.json
            fi
          fi

      - name: Bench (baseline)
        if: github.event_name == 'pull_request'
        run: |
          set -euxo pipefail
          if [ -f "${GITHUB_WORKSPACE}/perf-base.json" ]; then
            if [ -z "${BASELINE_SOURCE:-}" ]; then
              echo "BASELINE_SOURCE=existing perf-base.json" >> "${GITHUB_ENV}"
            fi
            echo "perf-base.json already present; skipping baseline bench."
            exit 0
          fi
          # Ensure we capture *only* baseline results. This also prevents stale criterion output
          # from a previous CI cache restore from being included in the baseline run.
          rm -rf "${CARGO_TARGET_DIR}/criterion"
          echo "BASELINE_SOURCE=benched base commit via git worktree" >> "${GITHUB_ENV}"
          git worktree add /tmp/nova-perf-base ${{ github.event.pull_request.base.sha }}
          pushd /tmp/nova-perf-base
          cargo bench --locked -p nova-core --bench critical_paths
          cargo bench --locked -p nova-syntax --bench parse_java
          cargo bench --locked -p nova-format --bench format
          cargo bench --locked -p nova-refactor --bench refactor
          cargo bench --locked -p nova-classpath --bench index
          if [ -f crates/nova-ide/benches/completion.rs ]; then
            cargo bench --locked -p nova-ide --bench completion
          else
            echo "Skipping nova-ide completion benchmark (not present in baseline revision)."
          fi
          if [ -f crates/nova-fuzzy/benches/fuzzy.rs ]; then
            cargo bench --locked -p nova-fuzzy --bench fuzzy
          else
            echo "Skipping nova-fuzzy benchmark (not present in baseline revision)."
          fi
          if [ -f crates/nova-index/benches/symbol_search.rs ]; then
            cargo bench --locked -p nova-index --bench symbol_search
          else
            echo "Skipping nova-index symbol_search benchmark (not present in baseline revision)."
          fi
          cargo run --locked -p nova-cli --release -- perf capture \
            --criterion-dir "${CARGO_TARGET_DIR}/criterion" \
            --out "${GITHUB_WORKSPACE}/perf-base.json"
          popd

      - name: Bench (current)
        run: |
          set -euxo pipefail
          # Ensure we capture *only* current results. Without this, benchmarks that were removed
          # on the current branch could leave stale `target/criterion/**/new/sample.json` files
          # from the baseline run, which would get incorrectly picked up by `perf capture`.
          rm -rf "${CARGO_TARGET_DIR}/criterion"
          cargo bench --locked -p nova-core --bench critical_paths
          cargo bench --locked -p nova-syntax --bench parse_java
          cargo bench --locked -p nova-format --bench format
          cargo bench --locked -p nova-refactor --bench refactor
          cargo bench --locked -p nova-classpath --bench index
          cargo bench --locked -p nova-ide --bench completion
          cargo bench --locked -p nova-fuzzy --bench fuzzy
          cargo bench --locked -p nova-index --bench symbol_search
          cargo run --locked -p nova-cli --release -- perf capture \
            --criterion-dir "${CARGO_TARGET_DIR}/criterion" \
            --out perf-current.json

      - name: Compare
        if: github.event_name == 'pull_request'
        run: |
          # `nova perf compare` uses its exit code to signal regressions; don't use `-e`
          # so we can always write/upload report artifacts even when thresholds fail.
          set -uox pipefail
          set +e

          # Keep the existing behavior:
          # - write a markdown report and append it to `GITHUB_STEP_SUMMARY`
          # - exit nonzero when regressions exceed thresholds
          #
          # Additionally: write a machine-readable JSON report when supported.
          {
            echo "### Perf comparison"
            echo ""
            echo "- Baseline SHA: \`${{ github.event.pull_request.base.sha }}\`"
            echo "- Current SHA: \`${{ github.sha }}\`"
            echo "- Baseline source: ${BASELINE_SOURCE:-unknown}"
            echo ""
          } >> "${GITHUB_STEP_SUMMARY}"
          status=0

          # Determine which JSON output mode the CLI supports.
          help_file="$(mktemp)"
          cargo run --locked -p nova-cli --release -- perf compare --help >"${help_file}"
          json_out_supported=0
          format_supported=0
          thresholds_args=()
          if grep -q -- '--json-out' "${help_file}"; then
            json_out_supported=1
          elif grep -q -- '--format' "${help_file}"; then
            format_supported=1
          fi

          if grep -q -- '--thresholds-config' "${help_file}"; then
            thresholds_args=(--thresholds-config perf/thresholds.toml)
          elif grep -q -- '--config' "${help_file}"; then
            thresholds_args=(--config perf/thresholds.toml)
          fi
          rm -f "${help_file}"

          if [ "${json_out_supported}" = "1" ]; then
            # Prefer a single invocation that emits both markdown + JSON.
            cargo run --locked -p nova-cli --release -- perf compare \
              --baseline perf-base.json \
              --current perf-current.json \
              "${thresholds_args[@]}" \
              --markdown-out perf-report.md \
              --json-out perf-report.json
            status=$?
          elif [ "${format_supported}" = "1" ]; then
            # Fallback for CLIs that only support structured output via `--format`.
            cargo run --locked -p nova-cli --release -- perf compare \
              --baseline perf-base.json \
              --current perf-current.json \
              "${thresholds_args[@]}" \
              --markdown-out perf-report.md
            status_md=$?

            cargo run --locked -p nova-cli --release -- perf compare \
              --baseline perf-base.json \
              --current perf-current.json \
              "${thresholds_args[@]}" \
              --format json > perf-report.json
            status_json=$?

            status=$status_md
            if [ "$status_json" -gt "$status" ]; then
              status=$status_json
            fi
          else
            # Backwards-compat: write a placeholder JSON file when the CLI can't emit JSON yet.
            cargo run --locked -p nova-cli --release -- perf compare \
              --baseline perf-base.json \
              --current perf-current.json \
              "${thresholds_args[@]}" \
              --markdown-out perf-report.md
            status=$?

            printf '{"error":"nova perf compare did not emit JSON (no supported JSON output flags)","exit_code":%s}\n' "${status}" > perf-report.json
          fi

          # Ensure we always have both report artifacts to upload, even if `cargo run` fails
          # before the CLI has a chance to write them.
          if [ ! -s perf-report.md ]; then
            {
              echo "## Nova performance report"
              echo ""
              echo "perf compare exited with status ${status} and did not produce perf-report.md."
              echo "See the job logs for details."
            } > perf-report.md
          fi

          if [ ! -s perf-report.json ]; then
            printf '{"error":"perf compare did not produce perf-report.json","exit_code":%s}\n' "${status}" > perf-report.json
            if [ "${status}" = "0" ]; then
              # If the compare succeeded but didn't emit JSON, that's a workflow/CLI contract bug.
              status=2
            fi
          fi

          # Make sure the summary step doesn't fail if the report couldn't be written.
          if [ -f perf-report.md ]; then
            cat perf-report.md >> "${GITHUB_STEP_SUMMARY}"
          else
            echo "perf compare failed (exit ${status}) and did not produce perf-report.md" >> "${GITHUB_STEP_SUMMARY}"
          fi

          exit $status

      - name: Cargo.lock unchanged
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          if ! git diff --exit-code -- Cargo.lock; then
            echo "Cargo.lock changed during CI; run cargo update locally and commit Cargo.lock."
            exit 1
          fi

      - name: Upload baseline (main)
        if: github.event_name == 'push' && success()
        uses: actions/upload-artifact@v4
        with:
          # Stable artifact name so other workflows can download the latest `main` baseline.
          name: perf-baseline-main
          if-no-files-found: error
          path: perf-current.json

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: perf-results
          if-no-files-found: ignore
          path: |
            perf-base.json
            perf-current.json
            perf-report.md
            perf-report.json
