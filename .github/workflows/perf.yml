name: perf

on:
  pull_request:
  push:
    branches:
      - main

permissions:
  contents: read
  actions: read

jobs:
  perf:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    env:
      # Use a shared target dir so the baseline + current worktrees can reuse build artifacts.
      CARGO_TARGET_DIR: ${{ github.workspace }}/target
      # Improves CI triage when `nova` panics or `anyhow` captures a backtrace.
      RUST_BACKTRACE: "1"

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Rust cache
        uses: Swatinem/rust-cache@v2

      # Optional optimization: reuse a cached baseline run from the `perf` workflow on `main`
      # instead of benching the base commit via a worktree.
      - name: Find cached baseline run (main)
        if: github.event_name == 'pull_request' && github.event.pull_request.base.ref == 'main' && github.event.pull_request.head.repo.full_name == github.repository
        id: baseline-run
        env:
          GH_TOKEN: ${{ github.token }}
          BASE_SHA: ${{ github.event.pull_request.base.sha }}
        run: |
          set -euo pipefail
          api_url="${GITHUB_API_URL}/repos/${GITHUB_REPOSITORY}/actions/workflows/perf.yml/runs?branch=main&per_page=50"
          run_id="$(
            curl -fsSL \
              -H "Authorization: Bearer ${GH_TOKEN}" \
              -H "Accept: application/vnd.github+json" \
              "${api_url}" | \
              python -c 'import json,os,sys; base=os.environ["BASE_SHA"]; data=json.load(sys.stdin); print(next((str(run["id"]) for run in data.get("workflow_runs", []) if run.get("head_sha")==base and run.get("conclusion")=="success"), ""), end="")' \
              || true
          )"

          if [ -n "${run_id}" ]; then
            echo "run_id=${run_id}" >> "${GITHUB_OUTPUT}"
          else
            echo "No cached baseline artifact found for ${BASE_SHA}; benching base commit instead."
          fi

      - name: Download cached baseline artifact (main)
        if: github.event_name == 'pull_request' && steps.baseline-run.outputs.run_id != ''
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: perf-baseline-main
          path: baseline
          repository: ${{ github.repository }}
          run-id: ${{ steps.baseline-run.outputs.run_id }}

      - name: Prepare baseline from artifact
        if: github.event_name == 'pull_request'
        env:
          BASELINE_RUN_ID: ${{ steps.baseline-run.outputs.run_id }}
        run: |
          set -euxo pipefail
          if [ -f baseline/perf-current.json ]; then
            cp baseline/perf-current.json perf-base.json
            echo "BASELINE_SOURCE=perf-baseline-main (run-id ${BASELINE_RUN_ID})" >> "${GITHUB_ENV}"
          fi

      - name: Bench (baseline)
        if: github.event_name == 'pull_request'
        run: |
          set -euxo pipefail
          if [ -f "${GITHUB_WORKSPACE}/perf-base.json" ]; then
            if [ -z "${BASELINE_SOURCE:-}" ]; then
              echo "BASELINE_SOURCE=existing perf-base.json" >> "${GITHUB_ENV}"
            fi
            echo "perf-base.json already present; skipping baseline bench."
            exit 0
          fi
          echo "BASELINE_SOURCE=benched base commit via git worktree" >> "${GITHUB_ENV}"
          git worktree add /tmp/nova-perf-base ${{ github.event.pull_request.base.sha }}
          pushd /tmp/nova-perf-base
          cargo bench -p nova-core --bench critical_paths
          cargo bench -p nova-syntax --bench parse_java
          cargo bench -p nova-format --bench format
          cargo bench -p nova-refactor --bench refactor
          cargo bench -p nova-classpath --bench index
          cargo run -p nova-cli --release -- perf capture \
            --criterion-dir "${CARGO_TARGET_DIR}/criterion" \
            --out "${GITHUB_WORKSPACE}/perf-base.json"
          popd

      - name: Bench (current)
        run: |
          set -euxo pipefail
          cargo bench -p nova-core --bench critical_paths
          cargo bench -p nova-syntax --bench parse_java
          cargo bench -p nova-format --bench format
          cargo bench -p nova-refactor --bench refactor
          cargo bench -p nova-classpath --bench index
          cargo run -p nova-cli --release -- perf capture \
            --criterion-dir "${CARGO_TARGET_DIR}/criterion" \
            --out perf-current.json

      - name: Compare
        if: github.event_name == 'pull_request'
        run: |
          # `nova perf compare` uses its exit code to signal regressions; don't use `-e`
          # so we can always write/upload report artifacts even when thresholds fail.
          set -uox pipefail
          set +e

          # Keep the existing behavior:
          # - write a markdown report and append it to `GITHUB_STEP_SUMMARY`
          # - exit nonzero when regressions exceed thresholds
          #
          # Additionally: write a machine-readable JSON report when supported.
          {
            echo "### Perf comparison"
            echo ""
            echo "- Baseline SHA: \`${{ github.event.pull_request.base.sha }}\`"
            echo "- Current SHA: \`${{ github.sha }}\`"
            echo "- Baseline source: ${BASELINE_SOURCE:-unknown}"
            echo ""
          } >> "${GITHUB_STEP_SUMMARY}"
          status=0

          # Determine which JSON output mode the CLI supports.
          help_file="$(mktemp)"
          cargo run -p nova-cli --release -- perf compare --help >"${help_file}"
          json_out_supported=0
          format_supported=0
          if grep -q -- '--json-out' "${help_file}"; then
            json_out_supported=1
          elif grep -q -- '--format' "${help_file}"; then
            format_supported=1
          fi
          rm -f "${help_file}"

          if [ "${json_out_supported}" = "1" ]; then
            # Prefer a single invocation that emits both markdown + JSON.
            cargo run -p nova-cli --release -- perf compare \
              --baseline perf-base.json \
              --current perf-current.json \
              --config perf/thresholds.toml \
              --markdown-out perf-report.md \
              --json-out perf-report.json
            status=$?
          elif [ "${format_supported}" = "1" ]; then
            # Fallback for CLIs that only support structured output via `--format`.
            cargo run -p nova-cli --release -- perf compare \
              --baseline perf-base.json \
              --current perf-current.json \
              --config perf/thresholds.toml \
              --markdown-out perf-report.md
            status_md=$?

            cargo run -p nova-cli --release -- perf compare \
              --baseline perf-base.json \
              --current perf-current.json \
              --config perf/thresholds.toml \
              --format json > perf-report.json
            status_json=$?

            status=$status_md
            if [ "$status_json" -gt "$status" ]; then
              status=$status_json
            fi
          else
            # Backwards-compat: write a placeholder JSON file when the CLI can't emit JSON yet.
            cargo run -p nova-cli --release -- perf compare \
              --baseline perf-base.json \
              --current perf-current.json \
              --config perf/thresholds.toml \
              --markdown-out perf-report.md
            status=$?

            cat > perf-report.json <<'EOF'
          {"error":"nova perf compare did not emit JSON (no supported JSON output flags)"}
          EOF
          fi

          # Ensure we always have both report artifacts to upload, even if `cargo run` fails
          # before the CLI has a chance to write them.
          if [ ! -f perf-report.md ]; then
            cat > perf-report.md <<EOF
          ## Nova performance report

          perf compare exited with status ${status} and did not produce perf-report.md.
          See the job logs for details.
          EOF
          fi

          if [ ! -f perf-report.json ]; then
            cat > perf-report.json <<EOF
          {"error":"perf compare did not produce perf-report.json","exit_code":${status}}
          EOF
            if [ "${status}" = "0" ]; then
              # If the compare succeeded but didn't emit JSON, that's a workflow/CLI contract bug.
              status=2
            fi
          fi

          # Make sure the summary step doesn't fail if the report couldn't be written.
          if [ -f perf-report.md ]; then
            cat perf-report.md >> "${GITHUB_STEP_SUMMARY}"
          else
            echo "perf compare failed (exit ${status}) and did not produce perf-report.md" >> "${GITHUB_STEP_SUMMARY}"
          fi

          exit $status

      - name: Upload baseline (main)
        if: github.event_name == 'push'
        uses: actions/upload-artifact@v4
        with:
          # Stable artifact name so other workflows can download the latest `main` baseline.
          name: perf-baseline-main
          if-no-files-found: error
          path: perf-current.json

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: perf-results
          if-no-files-found: ignore
          path: |
            perf-base.json
            perf-current.json
            perf-report.md
            perf-report.json
