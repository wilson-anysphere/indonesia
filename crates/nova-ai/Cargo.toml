[package]
name = "nova-ai"
version.workspace = true
edition = "2021"

[dependencies]
async-stream = "0.3"
async-trait = "0.1"
futures = "0.3"
futures-timer = "3"
hnsw_rs = { version = "0.3", optional = true }
rayon = { version = "1", optional = true }
fastembed = { version = "5.8.1", optional = true, default-features = false, features = ["ort-download-binaries", "hf-hub-rustls-tls"] }
nova-core = { path = "../nova-core" }
nova-fuzzy = { path = "../nova-fuzzy" }
nova-syntax = { path = "../nova-syntax" }
nova-db = { path = "../nova-db" }
nova-metrics = { path = "../nova-metrics" }
globset = "0.4"
reqwest = { version = "0.12", default-features = false, features = ["json", "stream", "rustls-tls"] }
serde = { version = "1", features = ["derive"] }
serde_json = "1"
sha2 = "0.10"
thiserror = "1"
tokio = { version = "1", features = ["macros", "rt-multi-thread", "sync", "time"] }
tokio-util = "0.7"
tracing = "0.1"
url = "2"
once_cell = "1"
regex = "1"
nova-config = { path = "../nova-config" }
llama-cpp-2 = { version = "0.1.131", optional = true, features = ["sampler"] }
unicode-ident = "1"

[features]
# Enables in-process local inference using GGUF models via llama.cpp.
local-llm = ["dep:llama-cpp-2"]
embeddings = ["dep:hnsw_rs", "dep:rayon", "reqwest/blocking"]
embeddings-local = ["embeddings", "dep:fastembed"]

[dev-dependencies]
httpmock = "0.7"
hyper = { version = "0.14", features = ["server", "http1", "tcp", "stream"] }
tempfile = "3"
tracing-subscriber = "0.3"
walkdir = "2"
